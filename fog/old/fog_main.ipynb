{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fog_season(data_frame):\n",
    "    plt.clf()\n",
    "    month_list = range(1, 13)\n",
    "    fog_count = []\n",
    "\n",
    "    for m in month_list:\n",
    "        fog_count.append(\n",
    "            data_frame[(data_frame['wx'].str.contains('Fog')) & (data_frame.obs_time.month == m)].shape[0])\n",
    "\n",
    "    station_name = data_frame.iloc[0].station\n",
    "    plt.bar(month_list, fog_count)\n",
    "    # plt.xticks(m)\n",
    "    plt.title('{} Airport - Fog reports per month'.format(station_name))\n",
    "    plt.savefig('{}_fog_seasonality.png'.format(station_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n",
      "Index([u'station_id', u' obs_time', u' wx', u' rh', u' t', u' td', u' wdir',\n",
      "       u' wspeed', u' pressure'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for i in glob.iglob('./station_csvs/*csv'):\n",
    "    df = pd.read_csv(i)\n",
    "    print df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rearrange(data_frame):\n",
    "    \n",
    "    # replacing 'Fog', 'Partial Fog' etc occurrences for 1's, other for 0's\n",
    "    wx_types = list(data_frame.presentwx.unique())\n",
    "    fog_types = []\n",
    "\n",
    "    for i in wx_types:\n",
    "        if 'FG' in i:\n",
    "            fog_types.append(i)\n",
    "            wx_types.remove(i)\n",
    "\n",
    "    data_frame.presentwx = data_frame.presentwx.replace(fog_types, 1)\n",
    "    data_frame.presentwx = data_frame.presentwx.replace(wx_types, 0)\n",
    "    \n",
    "    # removing consecutive observations of fog\n",
    "    del_indexes = []\n",
    "    for index, row in data_frame[0:-1].iterrows():\n",
    "        if row.presentwx == 1 and data_frame.iloc[index + 1].presentwx == 1:\n",
    "            del_indexes.append(index)\n",
    "\n",
    "    data_frame = data_frame.drop(data_frame.index[del_indexes])\n",
    "    # placing features from X hours before in current line\n",
    "    lead_hours = 6\n",
    "\n",
    "    tmpf = (lead_hours+1)*['M']\n",
    "    dwpf = (lead_hours+1)*['M']\n",
    "    relh = (lead_hours+1)*['M']\n",
    "    drct = (lead_hours+1)*['M']\n",
    "    sknt = (lead_hours+1)*['M']\n",
    "    alti = (lead_hours+1)*['M']\n",
    "\n",
    "    for index, row in data_frame[lead_hours:-1].iterrows():\n",
    "        valid_time = row['valid'] - pd.Timedelta(hours=lead_hours, minutes=row['valid'].minute)\n",
    "        lead_row = data_frame.loc[data_frame['valid'] == valid_time]\n",
    "\n",
    "        try:\n",
    "            tmpf.append(lead_row['tmpf'].values[0])\n",
    "        except:\n",
    "            tmpf.append('M')\n",
    "\n",
    "        try:\n",
    "            dwpf.append(lead_row['dwpf'].values[0])\n",
    "        except:\n",
    "            dwpf.append('M')\n",
    "\n",
    "        try:\n",
    "            relh.append(lead_row['relh'].values[0])\n",
    "        except:\n",
    "            relh.append('M')\n",
    "\n",
    "        try:\n",
    "            drct.append(lead_row['drct'].values[0])\n",
    "        except:\n",
    "            drct.append('M')\n",
    "\n",
    "        try:\n",
    "            sknt.append(lead_row['sknt'].values[0])\n",
    "        except:\n",
    "            sknt.append('M')\n",
    "\n",
    "        try:\n",
    "            alti.append(lead_row['alti'].values[0])\n",
    "        except:\n",
    "            alti.append('M')\n",
    "\n",
    "    data_frame['tmpf_{}h'.format(lead_hours)] = np.asarray(tmpf)\n",
    "    data_frame['dwpf_{}h'.format(lead_hours)] = np.asarray(dwpf)\n",
    "    data_frame['relh_{}h'.format(lead_hours)] = np.asarray(relh)\n",
    "    data_frame['drct_{}h'.format(lead_hours)] = np.asarray(drct)\n",
    "    data_frame['sknt_{}h'.format(lead_hours)] = np.asarray(sknt)\n",
    "    data_frame['alti_{}h'.format(lead_hours)] = np.asarray(alti)\n",
    "\n",
    "    # removing some columns, sending 'presentwx' to the right\n",
    "    cols = data_frame.columns.tolist()\n",
    "    for i in ['tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'alti', 'vsby']:\n",
    "        cols.remove(i)\n",
    "    cols.append(cols.pop(cols.index('presentwx')))\n",
    "    data_frame = data_frame[cols]\n",
    "    \n",
    "    # removing 'M' rows\n",
    "    data_frame = data_frame.replace('M', np.nan)\n",
    "    data_frame = data_frame.dropna(how='any')\n",
    "    \n",
    "    # to remove sparcicity, drop months outside Fog Season (april-september) or drop random 'no fog' rows\n",
    "#     data_frame = data_frame.drop(data_frame[(data_frame['valid'].dt.month < 4)].index)\n",
    "#     data_frame = data_frame.drop(data_frame[(data_frame['valid'].dt.month > 7)].index)\n",
    "    \n",
    "#     no_fog = data_frame[(data_frame['valid'].dt.month > 7)].index\n",
    "#     no_fog = np.random.choice(no_fog,int(np.shape(no_fog)[0]*0.2),replace=False)\n",
    "\n",
    "    data_frame = data_frame.drop(data_frame.query('presentwx == 0').sample(frac=.97).index)\n",
    "    \n",
    "    \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column_names = ['station_id', 'obs_time', 'wx', 'rh', 't', 'td', 'wdir', 'wspeed', 'pressure']\n",
    "column_names = ['station', 'valid', 'tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'alti', 'mslp', 'vsby', 'gust',\n",
    "                'skyc1', 'skyc2', 'skyc3', 'skyc4', 'skyl1', 'skyl2', 'skyl3', 'skyl4', 'presentwx', 'metar']\n",
    "\n",
    "usecols = ['station', 'valid', 'tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'alti', 'vsby', 'presentwx']\n",
    "\n",
    "dataset = pd.read_csv('./SBPA.csv', names=column_names, skiprows=6, parse_dates=['valid'], usecols=usecols,\n",
    "                 date_parser=lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M'), low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50941, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = rearrange(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1612, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([153, 102, 158, 134, 169, 162, 161, 120, 114, 209]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "np.histogram(dataset['valid'].dt.month,bins=[1,2,3,4,5,6,7,8,9,10,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading Features and Labels\n",
    "X = dataset.iloc[:, 2:8].values\n",
    "y = dataset.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generatin Training, Validation and Testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igoro/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "# X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[264  11]\n",
      " [ 44   4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "# classifier = RandomForestClassifier(n_estimators=30, criterion='entropy', random_state=0)\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8297213622291022"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(264.+4.)/np.sum(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([900, 131]), array([ 0. ,  0.5,  1. ]))\n",
      "(array([221,  37]), array([ 0. ,  0.5,  1. ]))\n",
      "(array([275,  48]), array([ 0. ,  0.5,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "print(np.histogram(y_train,bins=[0,.5,1]))\n",
    "print(np.histogram(y_val,bins=[0,.5,1]))\n",
    "print(np.histogram(y_test,bins=[0,.5,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(8,  activation='relu', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Adding hidden layers\n",
    "classifier.add(Dense(16,  activation='relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "\n",
    "classifier.add(Dense(32,  activation='relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "\n",
    "classifier.add(Dense(64,  activation='relu'))\n",
    "classifier.add(Dropout(0.3))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1031 samples, validate on 258 samples\n",
      "Epoch 1/20\n",
      "1024/1031 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.8477Epoch 00000: val_loss improved from inf to 0.36792, saving model to weights1.hdf5\n",
      "1031/1031 [==============================] - 4s - loss: 0.6574 - acc: 0.8448 - val_loss: 0.3679 - val_acc: 0.8566\n",
      "Epoch 2/20\n",
      "1021/1031 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.8609Epoch 00001: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.5244 - acc: 0.8623 - val_loss: 0.7313 - val_acc: 0.8566\n",
      "Epoch 3/20\n",
      "1025/1031 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.8634Epoch 00002: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.5481 - acc: 0.8632 - val_loss: 0.5235 - val_acc: 0.8566\n",
      "Epoch 4/20\n",
      "1021/1031 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8619Epoch 00003: val_loss did not improve\n",
      "1031/1031 [==============================] - 6s - loss: 0.5147 - acc: 0.8632 - val_loss: 0.4180 - val_acc: 0.8566\n",
      "Epoch 5/20\n",
      "1030/1031 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8544Epoch 00004: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 0.5516 - acc: 0.8545 - val_loss: 0.5332 - val_acc: 0.8566\n",
      "Epoch 6/20\n",
      "1027/1031 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8491Epoch 00005: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.5916 - acc: 0.8497 - val_loss: 0.4080 - val_acc: 0.8566\n",
      "Epoch 7/20\n",
      "1029/1031 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8484Epoch 00006: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 0.5854 - acc: 0.8487 - val_loss: 0.5261 - val_acc: 0.8566\n",
      "Epoch 8/20\n",
      "1029/1031 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.8397Epoch 00007: val_loss did not improve\n",
      "1031/1031 [==============================] - 6s - loss: 0.6307 - acc: 0.8400 - val_loss: 0.5274 - val_acc: 0.8566\n",
      "Epoch 9/20\n",
      "1026/1031 [============================>.] - ETA: 0s - loss: 0.7889 - acc: 0.8216Epoch 00008: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 0.7860 - acc: 0.8215 - val_loss: 0.5172 - val_acc: 0.8566\n",
      "Epoch 10/20\n",
      "1023/1031 [============================>.] - ETA: 0s - loss: 0.8152 - acc: 0.8201Epoch 00009: val_loss improved from 0.36792 to 0.35953, saving model to weights1.hdf5\n",
      "1031/1031 [==============================] - 5s - loss: 0.8090 - acc: 0.8215 - val_loss: 0.3595 - val_acc: 0.8450\n",
      "Epoch 11/20\n",
      "1021/1031 [============================>.] - ETA: 0s - loss: 0.8969 - acc: 0.8266Epoch 00010: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.8911 - acc: 0.8264 - val_loss: 0.3845 - val_acc: 0.8140\n",
      "Epoch 12/20\n",
      "1030/1031 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.8233Epoch 00011: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.7377 - acc: 0.8225 - val_loss: 0.4701 - val_acc: 0.8566\n",
      "Epoch 13/20\n",
      "1023/1031 [============================>.] - ETA: 0s - loss: 0.9124 - acc: 0.8260Epoch 00012: val_loss did not improve\n",
      "1031/1031 [==============================] - 4s - loss: 0.9091 - acc: 0.8264 - val_loss: 0.3870 - val_acc: 0.8372\n",
      "Epoch 14/20\n",
      "1028/1031 [============================>.] - ETA: 0s - loss: 0.8766 - acc: 0.8298Epoch 00013: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 0.8747 - acc: 0.8303 - val_loss: 1.2806 - val_acc: 0.8566\n",
      "Epoch 15/20\n",
      "1025/1031 [============================>.] - ETA: 0s - loss: 0.9368 - acc: 0.8234Epoch 00014: val_loss did not improve\n",
      "1031/1031 [==============================] - 6s - loss: 0.9322 - acc: 0.8244 - val_loss: 0.6991 - val_acc: 0.8566\n",
      "Epoch 16/20\n",
      "1024/1031 [============================>.] - ETA: 0s - loss: 0.8130 - acc: 0.8145Epoch 00015: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 0.8080 - acc: 0.8157 - val_loss: 1.1602 - val_acc: 0.8566\n",
      "Epoch 17/20\n",
      "1028/1031 [============================>.] - ETA: 0s - loss: 1.1879 - acc: 0.8132Epoch 00016: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 1.1846 - acc: 0.8138 - val_loss: 0.9309 - val_acc: 0.8566\n",
      "Epoch 18/20\n",
      "1026/1031 [============================>.] - ETA: 0s - loss: 1.0623 - acc: 0.8333Epoch 00017: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 1.0608 - acc: 0.8332 - val_loss: 1.0195 - val_acc: 0.8566\n",
      "Epoch 19/20\n",
      "1022/1031 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.8337Epoch 00018: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 1.0992 - acc: 0.8332 - val_loss: 0.7761 - val_acc: 0.8566\n",
      "Epoch 20/20\n",
      "1023/1031 [============================>.] - ETA: 0s - loss: 1.1502 - acc: 0.8338Epoch 00019: val_loss did not improve\n",
      "1031/1031 [==============================] - 5s - loss: 1.1480 - acc: 0.8332 - val_loss: 0.7722 - val_acc: 0.8566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c21e5c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the classifier\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights1.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "classifier.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=epochs, batch_size=1, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[275   0]\n",
      " [ 48   0]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "classifier.load_weights('weights1.hdf5')\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
